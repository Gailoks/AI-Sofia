import DatasetIterator as dsi
import Tokens as tk
import Dataset as ds
import ServiceTokens as st
import torch

device = 'cpu'
batch_size = 10
dataset = ds.load()
tokens = tk.TokenDictionary.load(".aistate/tokens.json")
tokenizer = tk.Tokenizer(tokens)
service_tokens = st.ServiceTokens(tokenizer.count_tokens())
train = []
target  = []
mask = []
for sample in dataset.listPairs():
    tq = list(tokenizer.tokenize(sample.question))
    ta = list(tokenizer.tokenize(sample.answer))
    train += tq + [service_tokens.get(st.STI_ROLE)] + ta
    target += tq + ta + [service_tokens.get(st.STO_END)]
    mask += len(tq)*[0] + len(ta)*[1]

def partition(l:iter, split_size:int)->iter:
    for i in range(0, len(l), split_size):
        yield l[i:i + split_size]

train = list(partition(train, 4))
target = list(partition(target, 4))
mask = list(partition(mask,4))

q = torch.LongTensor(train[0:2])
a = torch.LongTensor(target[0:2])
m = torch.BoolTensor(mask[0:2])
am = torch.masked_select(a,m)
something = torch.LongTensor([[[1,2,3,4,5,6,7,8], [9,10,11,12,13,14,15,16], [17,18,19,20,21,22,23,24]]]).permute(1,0,2)#3 1 8  3 - batch_len, 1 - batch_size, 8 - vocab_size
something = torch.masked_select(something,m.view(1,-1,1))
print(something.view(8,3,-1))